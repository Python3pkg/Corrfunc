/* File: xi_kernels.c.src */
/*
  This file is a part of the Corrfunc package
  Copyright (C) 2015-- Manodeep Sinha (manodeep@gmail.com)
  License: MIT LICENSE. See LICENSE file under the top-level
  directory at https://github.com/manodeep/Corrfunc/
*/


#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <stdint.h>

#include "function_precision.h"

#if defined(__AVX__)
#include "avx_calls.h"

static inline int xi_avx_intrinsics_DOUBLE(DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const int64_t N1,
                                           DOUBLE *x2, DOUBLE *y2, DOUBLE *z2, const int64_t N2, const int same_cell,
                                           const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                           const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap
                                           ,DOUBLE *src_ravg
                                           ,uint64_t *src_npairs)
{
    uint64_t npair[nbin];
    AVX_FLOATS m_rupp_sqr[nbin];
    for(int i=0;i<nbin;i++) {
        npair[i] = 0;
        m_rupp_sqr[i] = AVX_SET_FLOAT(rupp_sqr[i]);
    }

    const int32_t need_ravg = src_ravg != NULL;
    DOUBLE ravg[nbin];
    AVX_FLOATS m_kbin[nbin];
    if(need_ravg) {
        for(int i=0;i<nbin;i++) {
            ravg[i] = ZERO;
            m_kbin[i] = AVX_SET_FLOAT((DOUBLE) i);
        }
    }

    
    for(int64_t i=0;i<N1;i++) {
        const DOUBLE x1pos = *x1++ + off_xwrap;
        const DOUBLE y1pos = *y1++ + off_ywrap;
        const DOUBLE z1pos = *z1++ + off_zwrap;
        

        DOUBLE *localz2 = (DOUBLE *) z2;
        int64_t j=0;
        if(same_cell == 1) {
            j = i+1;
            localz2 += j;
        } else {
            while(j < N2){
                const DOUBLE dz = *localz2++-z1pos;
                if(dz > -rmax) break;
                j++;
            }
            localz2--;
        }
        DOUBLE *localx2 = x2 + j;
        DOUBLE *localy2 = y2 + j;

        for(;j<=(N2-AVX_NVEC);j+=AVX_NVEC) {
            const AVX_FLOATS m_xpos    = AVX_SET_FLOAT(x1pos);
            const AVX_FLOATS m_ypos    = AVX_SET_FLOAT(y1pos);
            const AVX_FLOATS m_zpos    = AVX_SET_FLOAT(z1pos);
            
            union int8 {
                AVX_INTS m_ibin;
                int ibin[AVX_NVEC];
            };
            union int8 union_rbin;
            
            union float8{
                AVX_FLOATS m_Dperp;
                DOUBLE Dperp[AVX_NVEC];
            };
            union float8 union_mDperp;

            const AVX_FLOATS m_x2 = AVX_LOAD_FLOATS_UNALIGNED(localx2);
            const AVX_FLOATS m_y2 = AVX_LOAD_FLOATS_UNALIGNED(localy2);
            const AVX_FLOATS m_z2 = AVX_LOAD_FLOATS_UNALIGNED(localz2);
            
            localx2 += AVX_NVEC;//this might actually exceed the allocated range but we will never dereference that
            localy2 += AVX_NVEC;
            localz2 += AVX_NVEC;

            const AVX_FLOATS m_rmax = AVX_SET_FLOAT(rmax);
            
            const AVX_FLOATS m_zdiff = AVX_SUBTRACT_FLOATS(m_z2,m_zpos);//z2[j:j+NVEC-1] - z1
            const AVX_FLOATS m_sqr_zdiff = AVX_SQUARE_FLOAT(m_zdiff);
            const AVX_FLOATS m_sqr_xdiff = AVX_SQUARE_FLOAT(AVX_SUBTRACT_FLOATS(m_xpos,m_x2));//(x0 - x[j])^2
            const AVX_FLOATS m_sqr_ydiff = AVX_SQUARE_FLOAT(AVX_SUBTRACT_FLOATS(m_ypos,m_y2));//(y0 - y[j])^2
            AVX_FLOATS r2  = AVX_ADD_FLOATS(m_sqr_xdiff,AVX_ADD_FLOATS(m_sqr_ydiff, m_sqr_zdiff));
            
            AVX_FLOATS m_mask_left;
            
            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than rmax, then
                //no future iteration in j can produce a zdiff value less than rmax.
                AVX_FLOATS m_mask_rmax = AVX_COMPARE_FLOATS(m_zdiff,m_rmax,_CMP_LT_OS);
                if(AVX_TEST_COMPARISON(m_mask_rmax) == 0) {
                    //None of the dz^2 values satisfies dz^2 < rmax^2
                    // => no pairs can be added -> continue and process the next NVEC
                    j=N2;
                    break;
                }
                
                const AVX_FLOATS m_pimax_mask = AVX_COMPARE_FLOATS(r2, m_rupp_sqr[nbin-1], _CMP_LT_OS);
                const AVX_FLOATS m_rmin_mask = AVX_COMPARE_FLOATS(r2, m_rupp_sqr[0], _CMP_GE_OS);
                const AVX_FLOATS m_r_mask = AVX_BITWISE_AND(m_pimax_mask,m_rmin_mask);
                
                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_rmin <= r2 < sqr_rmax
                m_mask_left = AVX_BITWISE_AND(m_mask_rmax,m_r_mask);
                
                //If not, continue with the next iteration of j-loop
                if(AVX_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }
                
                //There is some r2 that satisfies sqr_rmin <= r2 < sqr_rmax && 0.0 <= dz^2 < rmax^2.
                r2 = AVX_BLEND_FLOATS_WITH_MASK(m_rupp_sqr[nbin-1], r2, m_mask_left);
            }
            
            AVX_FLOATS m_rbin = AVX_SET_FLOAT(ZERO);
            if(need_ravg) {
                union_mDperp.m_Dperp = AVX_SQRT_FLOAT(r2);
            }

            //Loop backwards through nbins. m_mask_left contains all the points that are less than rmax
            for(int kbin=nbin-1;kbin>=1;kbin--) {
                const AVX_FLOATS m1 = AVX_COMPARE_FLOATS(r2,m_rupp_sqr[kbin-1],_CMP_GE_OS);
                const AVX_FLOATS m_bin_mask = AVX_BITWISE_AND(m1,m_mask_left);
                const int test2  = AVX_TEST_COMPARISON(m_bin_mask);
                npair[kbin] += AVX_BIT_COUNT_INT(test2);
                if(need_ravg) {
                    m_rbin = AVX_BLEND_FLOATS_WITH_MASK(m_rbin,m_kbin[kbin], m_bin_mask);
                }
                m_mask_left = AVX_COMPARE_FLOATS(r2,m_rupp_sqr[kbin-1],_CMP_LT_OS);
                const int test3 = AVX_TEST_COMPARISON(m_mask_left);
                if(test3 == 0) {
                    break;
                }
            }

            if(need_ravg) {
                union_rbin.m_ibin = AVX_TRUNCATE_FLOAT_TO_INT(m_rbin);
                //protect the unroll pragma in case compiler is not icc.
#if  __INTEL_COMPILER
#pragma unroll(AVX_NVEC)
#endif
                for(int jj=0;jj<AVX_NVEC;jj++) {
                    const int kbin = union_rbin.ibin[jj];
                    const DOUBLE r = union_mDperp.Dperp[jj];
                    ravg[kbin] += r;
                }
            }
        }//end of j-loop

        //remainder loop 
        for(;j<N2;j++){
            const DOUBLE dz = *localz2++ - z1pos;
            const DOUBLE dx = *localx2++ - x1pos;
            const DOUBLE dy = *localy2++ - y1pos;

            if(dz >= rmax) {
                j = N2;
                break;
            } 
            
            const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
            if(r2 >= sqr_rmax || r2 < sqr_rmin) {
                continue;
            }
            
            DOUBLE r;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures
            if(need_ravg) {
                r = SQRT(r2);
            }
            
            for(int kbin=nbin-1;kbin>=1;kbin--) {
                if(r2 >= rupp_sqr[kbin-1]) {
                    npair[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    break;
                }
            }
        }//remainder loop over cellstruct second
    }//loop over cellstruct first

    for(int i=0;i<nbin;i++) {
        src_npairs[i] += npair[i];
        if(need_ravg) {
            src_ravg[i]  += ravg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__AVX__



#if defined (__SSE4_2__)
#include "sse_calls.h"

static inline int xi_sse_intrinsics_DOUBLE(DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const int64_t N0,
                                           DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const int64_t N1, const int same_cell, 
                                           const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                           const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap
                                           ,DOUBLE *src_ravg
                                           ,uint64_t *src_npairs)
{
    uint64_t npairs[nbin];
    SSE_FLOATS m_rupp_sqr[nbin];
    for(int i=0;i<nbin;i++) {
        npairs[i] = ZERO;
        m_rupp_sqr[i] = SSE_SET_FLOAT(rupp_sqr[i]);
    }
    
    const int32_t need_ravg = src_ravg != NULL;
    DOUBLE ravg[nbin];
    SSE_FLOATS m_kbin[nbin];
    if(need_ravg) {
        for(int i=0;i<nbin;i++) {
            ravg[i] = ZERO;
            m_kbin[i] = SSE_SET_FLOAT((DOUBLE) i);
        }
    }


    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;


        DOUBLE *localz1 = (DOUBLE *) z1;
        int64_t j=0;
        if(same_cell == 1) {
            j = i+1;
            localz1 += j;
        } else {
            while(j < N1){
                const DOUBLE dz = *localz1++ - zpos;
                if(dz > -rmax) break;
                j++;
            }
            localz1--;
        }
        DOUBLE *localx1 = x1 + j;
        DOUBLE *localy1 = y1 + j;
        
		for(;j<=(N1 - SSE_NVEC);j+=SSE_NVEC){
            union int4{
                SSE_INTS m_ibin;
                int ibin[SSE_NVEC];
            };
            union int4 union_rbin;
            
            union float4{
                SSE_FLOATS m_Dperp;
                DOUBLE Dperp[SSE_NVEC];
            };
            union float4 union_mDperp;

            const SSE_FLOATS m_xpos = SSE_SET_FLOAT(xpos);
            const SSE_FLOATS m_ypos = SSE_SET_FLOAT(ypos);
            const SSE_FLOATS m_zpos = SSE_SET_FLOAT(zpos);

            const SSE_FLOATS m_x1 = SSE_LOAD_FLOATS_UNALIGNED(localx1);
			const SSE_FLOATS m_y1 = SSE_LOAD_FLOATS_UNALIGNED(localy1);
			const SSE_FLOATS m_z1 = SSE_LOAD_FLOATS_UNALIGNED(localz1);

			localx1 += SSE_NVEC;
			localy1 += SSE_NVEC;
			localz1 += SSE_NVEC;

            const SSE_FLOATS m_rmax = SSE_SET_FLOAT(rmax);
			const SSE_FLOATS m_sqr_rmax = SSE_SET_FLOAT(sqr_rmax);
			const SSE_FLOATS m_sqr_rmin = SSE_SET_FLOAT(sqr_rmin);
			

			const SSE_FLOATS m_sqr_xdiff = SSE_SQUARE_FLOAT(SSE_SUBTRACT_FLOATS(m_x1, m_xpos));
            const SSE_FLOATS m_sqr_ydiff = SSE_SQUARE_FLOAT(SSE_SUBTRACT_FLOATS(m_y1, m_ypos));
            const SSE_FLOATS m_zdiff = SSE_SUBTRACT_FLOATS(m_z1, m_zpos);
            const SSE_FLOATS m_sqr_zdiff = SSE_SQUARE_FLOAT(m_zdiff);
                        
            SSE_FLOATS r2  = SSE_ADD_FLOATS(m_sqr_zdiff,SSE_ADD_FLOATS(m_sqr_xdiff,m_sqr_ydiff));
            SSE_FLOATS m_mask_left;
			{
                const SSE_FLOATS m_pimax_mask = SSE_COMPARE_FLOATS_LT(m_zdiff,m_rmax);
                if(SSE_TEST_COMPARISON(m_pimax_mask) == 0) {
                    j = N1;
                    break;
                }
                
                const SSE_FLOATS m_rmin_mask = SSE_COMPARE_FLOATS_GE(r2, m_sqr_rmin);
                const SSE_FLOATS m_rmax_mask = SSE_COMPARE_FLOATS_LT(r2,m_sqr_rmax);
                const SSE_FLOATS m_r_mask = SSE_BITWISE_AND(m_rmin_mask, m_rmax_mask);
                m_mask_left = SSE_BITWISE_AND(m_pimax_mask, m_r_mask);
				if(SSE_TEST_COMPARISON(m_mask_left) == 0) {
					continue;
				}
				r2 = SSE_BLEND_FLOATS_WITH_MASK(m_sqr_rmax, r2, m_mask_left);
            }
                
            SSE_FLOATS m_rbin;
            if(need_ravg) {
                union_mDperp.m_Dperp = SSE_SQRT_FLOAT(r2);
                m_rbin = SSE_SET_FLOAT(ZERO);
            }

			for(int kbin=nbin-1;kbin>=1;kbin--) {
				SSE_FLOATS m1 = SSE_COMPARE_FLOATS_GE(r2,m_rupp_sqr[kbin-1]);
				SSE_FLOATS m_bin_mask = SSE_BITWISE_AND(m1,m_mask_left);
				m_mask_left = SSE_COMPARE_FLOATS_LT(r2,m_rupp_sqr[kbin-1]);
				int test2  = SSE_TEST_COMPARISON(m_bin_mask);
				npairs[kbin] += SSE_BIT_COUNT_INT(test2);
                if(need_ravg) {
                    m_rbin = SSE_BLEND_FLOATS_WITH_MASK(m_rbin,m_kbin[kbin], m_bin_mask);
                }
				int test3 = SSE_TEST_COMPARISON(m_mask_left);
				if(test3 == 0) {
					break;
				}
			}

            if(need_ravg) {
                union_rbin.m_ibin = SSE_TRUNCATE_FLOAT_TO_INT(m_rbin);
                //protect the unroll pragma in case compiler is not icc.
#if  __INTEL_COMPILER
#pragma unroll(SSE_NVEC)
#endif
                for(int jj=0;jj<SSE_NVEC;jj++) {
                    const int kbin = union_rbin.ibin[jj];
                    const DOUBLE r = union_mDperp.Dperp[jj];
                    ravg[kbin] += r;
                }
            }//need_ravg
		}//j-loop			

		for(;j<N1;j++) {
			const DOUBLE dx = *localx1++ - xpos;
			const DOUBLE dy = *localy1++ - ypos;
			const DOUBLE dz = *localz1++ - zpos;
            if(dz >= rmax) break;
            
			const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
			if(r2 >= sqr_rmax || r2 < sqr_rmin) continue;
            DOUBLE r;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures
            if(need_ravg) {
                r = SQRT(r2);
            }

			for(int kbin=nbin-1;kbin>=1;kbin--){
				if(r2 >= rupp_sqr[kbin-1]) {
					npairs[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    
					break;
				}
			}//searching for kbin loop
		}
    }

	for(int i=0;i<nbin;i++) {
		src_npairs[i] += npairs[i];
        if(need_ravg) {
            src_ravg[i] += ravg[i];
        }
	}

    return EXIT_SUCCESS;
}

#endif //__SSE4_2__

static inline int xi_fallback_DOUBLE(DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const int64_t N0,
                                     DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const int64_t N1, const int same_cell, 
                                     const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                     const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap
                                     ,DOUBLE *src_ravg
                                     ,uint64_t *src_npairs)
{
    /*----------------- FALLBACK CODE --------------------*/
    uint64_t npairs[nbin];
    for(int i=0;i<nbin;i++) {
        npairs[i]=0;
    }

    const int32_t need_ravg = src_ravg != NULL;
    DOUBLE ravg[nbin];
    if(need_ravg) {
        for(int i=0;i<nbin;i++) {
            ravg[i]= ZERO;
        }
    }


    /* naive implementation that is guaranteed to compile */
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;


        DOUBLE *localz1 = (DOUBLE *) z1;
        int64_t j=0;
        if(same_cell == 1) {
            j = i+1;
            localz1 += j;
        } else {
            while(j < N1){
                const DOUBLE dz = *localz1++ - zpos;
                if(dz > -rmax) break;
                j++;
            }
            localz1--;
        }
        DOUBLE *localx1 = x1 + j;
        DOUBLE *localy1 = y1 + j;
        
        for(;j<N1;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = *localz1++ - zpos;
            if(dz >=rmax) break;

            const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
            if(r2 >= sqr_rmax || r2 < sqr_rmin) continue;

            DOUBLE r;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures
            if(need_ravg) {
                r = SQRT(r2);
            }

            for(int kbin=nbin-1;kbin>=1;kbin--){
                if(r2 >= rupp_sqr[kbin-1]) {
                    npairs[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    break;
                }
            }//searching for kbin loop                                                               
        }
    }

    for(int i=0;i<nbin;i++) {
        src_npairs[i] += npairs[i];
        if(need_ravg) {
            src_ravg[i] += ravg[i];
        }
    }
    /*----------------- FALLBACK CODE --------------------*/
    return EXIT_SUCCESS;
}
